{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ab69d8",
   "metadata": {},
   "source": [
    "# Neural Network Quantum States\n",
    "\n",
    "Neural network quantum states (NQS) represent a powerful approach to the quantum many-body problem, leveraging the universal approximation capabilities of neural networks to parametrize complex wavefunctions. In this framework, a neural network encodes the wavefunction amplitude $\\psi_\\theta(\\mathbf{x})$ for quantum configurations $\\mathbf{x}$, with network parameters $\\theta$ optimized via variational Monte Carlo methods.\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- **Expressive power**: Neural networks can efficiently represent highly entangled quantum states that would be intractable for traditional variational ansätze\n",
    "- **Systematic improvability**: Network capacity can be increased systematically by adding layers or units to achieve desired accuracy\n",
    "- **Flexibility**: The approach is agnostic to system dimensionality, lattice geometry, and Hamiltonian structure\n",
    "- **Scalability**: Modern automatic differentiation and GPU acceleration enable application to systems with hundreds of particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf431b",
   "metadata": {},
   "source": [
    "## Theory: Variational Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc16511",
   "metadata": {},
   "source": [
    "The variational Monte Carlo (VMC) method provides a framework for finding approximate ground states by minimizing the energy functional\n",
    "$$E[\\psi] = \\frac{\\langle \\psi | H | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle}$$\n",
    "over a parametrized family of wavefunctions $\\psi_\\theta$. For a given configuration $\\mathbf{x}$, we define the local energy as \n",
    "$$E_\\text{loc}(\\mathbf{x}) = \\sum_{\\mathbf{x}'} H_{\\mathbf{x}\\mathbf{x}'} \\frac{\\psi_\\theta(\\mathbf{x}')}{\\psi_\\theta(\\mathbf{x})} \\; ,$$\n",
    "which allows us to express the energy expectation value as\n",
    "$$E(\\theta) = \\frac{\\sum_{\\mathbf{x}} |\\psi_\\theta(\\mathbf{x})|^2 E_\\text{loc}(\\mathbf{x})}{\\sum_{\\mathbf{x}} |\\psi_\\theta(\\mathbf{x})|^2} \\; .$$\n",
    "This formulation is exact but intractable for large systems due to the exponential sum over configurations. Monte Carlo sampling resolves this issue by drawing configurations $\\mathbf{x}$ from the probability distribution\n",
    "$$ p_\\theta (\\mathbf{x}) = \\frac{|\\psi_\\theta(\\mathbf{x})|^2}{ \\sum_{\\mathbf{x}} | \\psi_\\theta(\\mathbf{x})|^2} \\; ,$$\n",
    "The energy then becomes\n",
    "$$E(\\theta) = \\mathbb{E}_{\\mathbf{x} \\sim p_\\theta}[E_\\text{loc}(\\mathbf{x})] \\; ,$$\n",
    "which we estimate via Monte Carlo as $E(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N E_\\text{loc}(\\mathbf{x}_i)$ with $\\mathbf{x}_i \\sim p_\\theta$. Metropolis-Hastings or other MCMC algorithms generate these samples efficiently without computing the norm explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef433f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c717ae3b",
   "metadata": {},
   "source": [
    "## The VMC gradient expression\n",
    "\n",
    "The gradient of the energy expectation value with respect to the parameters, $\\nabla_\\theta E(\\theta)$ is a key quantity to optimize the total energy. It can be expressed in terms of the local energy and the logarithmic derivatives of the wavefunction. The expression is given by\n",
    "$$ \\frac{\\partial E}{\\partial \\theta ^\\mu} = 2 \\text{Re} \\; \\mathbb{E}_{\\mathbf{x} \\sim |\\psi _\\theta|^2 } \\left[ \\mathcal{O} ^* _\\mu (\\mathbf{x}) \\left( E_\\text{loc} (x) - E \\right) \\right] $$\n",
    "\n",
    "where $\\mathcal{O}_\\mu (\\mathbf{x}) = \\frac{\\partial}{\\partial \\theta^\\mu} \\ln \\psi_\\theta(\\mathbf{x})$ are the logarithmic derivatives of the wavefunction with respect to the parameters. This expression allows us to compute the gradient using samples drawn from the probability distribution $p_\\theta(\\mathbf{x})$, enabling efficient optimization of the neural network parameters via stochastic gradient descent or other optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c0b56",
   "metadata": {},
   "source": [
    "### Excercise: Derive this expression by direct differentiation of the energy expectation value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba183c6d",
   "metadata": {},
   "source": [
    "## Common Architectures\n",
    "Several neural network architectures have been successfully employed as NQS ansätze:\n",
    "- **Restricted Boltzmann Machines (RBM)**: Early NQS models used RBMs to capture correlations via hidden units, demonstrating success on spin systems.\n",
    "- **Feedforward Neural Networks**: Fully connected networks with nonlinear activations provide flexible function approximators for wavefunctions.\n",
    "- **Convolutional Neural Networks (CNNs)**: CNNs exploit spatial locality and translational symmetry, making them well-suited for lattice models.\n",
    "- **Recurrent Neural Networks (RNNs)**: RNNs capture sequential correlations, useful for one-dimensional systems.\n",
    "- **Transformer Networks**: Attention-based models that can capture long-range correlations effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620f9a6",
   "metadata": {},
   "source": [
    "### Restricted Boltzmann Machines\n",
    "\n",
    "A Restricted Boltzmann Machine (RBM) is a generative stochastic neural network that can learn a **probability** distribution over its set of inputs. We think of it as a classical Boltzmann distribution defined on a bipartite graph of spins $s_i \\in \\{-1, 1\\}$. We think of it as \"physical\" spins coupled to some \"hidden\" spins $h_j \\in \\{-1, 1\\}$ that mediate correlations. The joint probability distribution is given by\n",
    "\n",
    "$$ p(\\mathbf{s}, \\mathbf{h}) \\propto \\exp \\left\\{ \\sum_i a_i s_i + \\sum_j b_j h_j + \\sum_{i,j} W_{ij} s_i h_j \\right\\} $$\n",
    "\n",
    "where $\\mathbf{s}$ are the visible units (spin configurations), $\\mathbf{h}$ are the hidden units, $a_i$ and $b_j$ are biases, $W_{ij}$ are weights connecting visible and hidden units. To get the probability of a visible configuration, we need marginalize over the hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9091c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18b650c8",
   "metadata": {},
   "source": [
    "## Excercise: The RBM state\n",
    "\n",
    "Show that the expression for the RBM probability $p(\\mathbf{s})$ after marginalizing over the hidden units is\n",
    "$$ p(\\mathbf{s}) = \\sum_{\\mathbf{h}} p(\\mathbf{s}, \\mathbf{h}) \\propto \\exp \\left\\{ \\sum_i a_i s_i \\right\\} \\times \\prod _k \\cosh \\left( \\sum_i W_{ik} s_i \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8155113",
   "metadata": {},
   "source": [
    "The RBM wavefunction for a spin configuration $\\mathbf{s}$ is given by the same expression. The only trick is that we need the wavefunction amplitude, not the probability. This is achieved by making the parameters complex-valued and pretending the RBM defines the wavefunction (\"square root\" of the underlying probability distribution) directly.\n",
    "\n",
    "$$ \\ln \\psi (\\mathbf{s}) =  \\sum_i a_i s_i + \\sum_k \\ln \\cosh \\left( \\sum_i W_{ik} s_i \\right) $$\n",
    "\n",
    "Implement the RBM wavefunction in JAX using the provided scaffolding. Be extra careful with the $\\ln \\cosh (\\cdot)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jaxtyping import Array, Float, Scalar, PyTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88384c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember -- JAX knows how to propagate through standard Python containers.\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "class RBMParams(NamedTuple):\n",
    "    a: Float[Array, \"n_visible\"]           # Visible biases\n",
    "    b: Float[Array, \"n_hidden\"]            # Hidden biases\n",
    "    W: Float[Array, \"n_visible n_hidden\"]  # Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rbm(params: RBMParams, s: Float[Array, \"n_visible\"]) -> Scalar:\n",
    "    pass # Your implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632bf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a357ca5",
   "metadata": {},
   "source": [
    "# Excercise: Transverse Field Ising Model\n",
    "\n",
    "Consider a Hamiltonian of the form:\n",
    "$$H = -J \\sum_{\\langle i,j \\rangle} \\sigma_i^z \\sigma_j^z - h \\sum_i \\sigma_i^x,$$\n",
    "where $\\sigma_i^z$ and $\\sigma_i^x$ are the Pauli operators acting on the $i$-th spin, and $J$ and $h$ are coupling constants. Write down the local energy expression needed for the Variational Monte Carlo optimization. Assume a 1D circular geometry (use periodic boundary conditions).\n",
    "\n",
    "Use the folowing JAX template to implement the local energy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa053bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfim_local_energy(\n",
    "    logpsi: Callable, params: PyTree, x: Float[Array, \"n_visible\"], h: Scalar, J: Scalar = 1.0\n",
    ") -> Scalar:\n",
    "    \"\"\"\n",
    "    Compute the local energy for the transverse field Ising model.\n",
    "\n",
    "    Parameters:\n",
    "    logpsi: function\n",
    "        The neural network wavefunction taking configurations x as input.\n",
    "    params: PyTree\n",
    "        The parameters of the neural network wavefunction.\n",
    "    x: array\n",
    "        The spin configuration (1D array of +1/-1).\n",
    "    h: float\n",
    "        Transverse field strength.\n",
    "    J: float\n",
    "        Coupling constant for the ZZ interaction.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The local energy E_loc(x).\n",
    "    \"\"\"\n",
    "    pass\n",
    "    # Your implementation here\n",
    "    # Of course, feel free to make any auxiliary functions as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cbc4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd5152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97d639c7",
   "metadata": {},
   "source": [
    "## The optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7732960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "SOLUTIONS_PATH = Path(\"..\").resolve() # Adjust as needed\n",
    "sys.path.append(str(SOLUTIONS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "plt.style.use(['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "from src.solutions import RBMParams, eval_rbm, tfim_local_energy, energy_value_and_grad\n",
    "from src.sampler import SpinSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spins = 10\n",
    "h = 0.2\n",
    "learning_rate = 0.01\n",
    "n_iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0130a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = RBMParams.initialize(n_spins, n_hidden=2*n_spins, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3abfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SpinSampler(dims=(10,), n_samples=128, n_chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, = jax.random.split(key, 1)\n",
    "samples = sampler(lambda x: eval_rbm(params, x), key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226ba75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def step(params, key):\n",
    "\n",
    "    samples = sampler(lambda x: eval_rbm(params, x), key).reshape(-1, n_spins)\n",
    "    eloc_fn = lambda *args: tfim_local_energy(*args, h=h, J=1.0)\n",
    "    energy, grads = energy_value_and_grad(eloc_fn, eval_rbm, params, samples)\n",
    "\n",
    "    params_ = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, grads\n",
    "    )\n",
    "\n",
    "    return energy, params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4be22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b56ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clock = perf_counter()\n",
    "energies = []\n",
    "\n",
    "for i in range(n_iters):\n",
    "\n",
    "    key_ = jax.random.fold_in(key, i)\n",
    "    energy, params = step(params, key_)\n",
    "    energies.append(energy.item())\n",
    "\n",
    "    if perf_counter() - clock > 2.0:\n",
    "        print(f\"Iteration {i}, Energy: {energy:.6f}\")\n",
    "        clock = perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = np.asarray(energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(energies, label=\"VMC Energy\")\n",
    "\n",
    "ax.set_xlabel(\"Iteration\", fontsize=14)\n",
    "ax.set_ylabel(\"Energy\", fontsize=14)\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4688d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
