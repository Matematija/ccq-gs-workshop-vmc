{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72e99199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fd6da",
   "metadata": {},
   "source": [
    "# JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c90d67",
   "metadata": {},
   "source": [
    "JAX is a Python library that combines NumPy's familiar array operations with automatic differentiation and hardware acceleration (GPU/TPU). At its core, JAX provides composable function transformations that enable efficient gradient computation, vectorization, and just-in-time compilation. For scientific computing, JAX's key innovation is making gradient-based optimization as simple as calling `grad(f)` on any function `f`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5f791",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "Automatic differentiation (AD) computes derivatives by systematically applying the chain rule to elementary operations. Given a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ composed of operations as\n",
    "$$\n",
    "f = f_N \\circ f_{N-1} \\circ \\cdots \\circ f_1 \\; ,\n",
    "$$\n",
    "AD efficiently evaluates the gradient $\\nabla f(\\mathbf{x})$ by accumulating partial derivatives through the chain rule as\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i} = \\frac{\\partial f_N}{\\partial f_{N-1}} \\frac{\\partial f_{N-1}}{\\partial f_{N-2}} \\cdots \\frac{\\partial f_{n+1}}{\\partial f_n} \\cdots \\frac{\\partial f_1}{\\partial x_i} \\; ,\n",
    "$$\n",
    "which is really just matrix multiplication of Jacobian matrices. Unlike numerical differentiation (finite differences) which suffers from truncation errors, or symbolic differentiation which can produce exponentially large expressions, AD computes exact derivatives at machine precision with computational cost proportional to evaluating $f$ itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e688f377",
   "metadata": {},
   "source": [
    "Let's look at a simple example of using JAX for automatic differentiation. Consider the function\n",
    "$$\n",
    "f(x | A, b) = \\sum _ i \\frac{1}{1 + e^{-(A x + b)_i}} \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99cd74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(params, x):\n",
    "    A, b = params\n",
    "    y = A @ x + b\n",
    "    return jax.nn.sigmoid(y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cbf961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness is a little different in JAX, we will use numpy for this simple example\n",
    "x = jnp.array(np.random.randn(3)) \n",
    "A = jnp.array(np.random.randn(10, 3))\n",
    "b = jnp.array(np.random.randn(10))\n",
    "\n",
    "f_val, f_grad = jax.value_and_grad(f)((A, b), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ca0e077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5.9789677, dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f((A, b), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f64cc89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5.9789677, dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "858c9b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[-7.4027687e-02,  2.1198890e-01,  8.7550213e-04],\n",
       "        [-5.8650743e-02,  1.6795482e-01,  6.9364387e-04],\n",
       "        [-5.4628942e-02,  1.5643781e-01,  6.4607931e-04],\n",
       "        [-7.3608056e-02,  2.1078722e-01,  8.7053928e-04],\n",
       "        [-5.6869060e-02,  1.6285272e-01,  6.7257247e-04],\n",
       "        [-4.5137696e-02,  1.2925828e-01,  5.3382933e-04],\n",
       "        [-4.8312403e-02,  1.3834949e-01,  5.7137560e-04],\n",
       "        [-5.3489557e-03,  1.5317502e-02,  6.3260421e-05],\n",
       "        [-5.6961965e-02,  1.6311875e-01,  6.7367119e-04],\n",
       "        [-7.2778299e-02,  2.0841110e-01,  8.6072605e-04]], dtype=float32),\n",
       " Array([0.24874207, 0.19707367, 0.18355992, 0.24733205, 0.191087  ,\n",
       "        0.15166818, 0.16233557, 0.01797315, 0.19139916, 0.24454398],      dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_grad # It is a tuple with gradients with respect to A and b !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933daef",
   "metadata": {},
   "source": [
    "JAX know how to propagate gradients through Python containers too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852cb71",
   "metadata": {},
   "source": [
    "## Automatic Vectorization with `vmap`\n",
    "\n",
    "Let's see how `vmap` automatically vectorizes functions. Suppose we want to evaluate $f$ at multiple input points $\\{x_1, x_2, \\ldots, x_N\\}$ simultaneously. Without `vmap`, we would need to manually loop or add batch dimensions to our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5334a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of inputs: shape (100, 3)\n",
    "x_batch = jnp.array(np.random.randn(100, 3))\n",
    "\n",
    "# Naive approach: loop over batch\n",
    "results_loop = jnp.array([f((A, b), x_i) for x_i in x_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2a574bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With vmap: automatically vectorize over the batch dimension\n",
    "# in_axes=(None, 0) means: don't map over params, but map over axis 0 of x\n",
    "f_batched = jax.vmap(f, in_axes=(None, 0))\n",
    "results_vmap = f_batched((A, b), x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e41aa264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match: True\n",
      "Output shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Verify they produce the same results\n",
    "print(f\"Results match: {jnp.allclose(results_loop, results_vmap)}\")\n",
    "print(f\"Output shape: {results_vmap.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2943d",
   "metadata": {},
   "source": [
    "The `in_axes` argument specifies which axes to map over: `None` means \"don't vectorize this argument\", while `0` means \"map over axis 0\". We can also compose transformations: `vmap(grad(f))` computes gradients for a batch of inputs in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b8a95d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient shape: (100, 3)  # One gradient per input\n"
     ]
    }
   ],
   "source": [
    "# Example: batched gradients\n",
    "grad_f_batched = jax.vmap(jax.grad(f, argnums=1), in_axes=(None, 0))\n",
    "gradients_batch = grad_f_batched((A, b), x_batch)\n",
    "print(f\"Gradient shape: {gradients_batch.shape}  # One gradient per input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702042df",
   "metadata": {},
   "source": [
    "## Just-In-Time Compilation with `jit`\n",
    "\n",
    "JAX can compile functions to optimized machine code using `jit` (just-in-time compilation). This traces the function with abstract values, optimizes the computation graph, and compiles it to XLA (Accelerated Linear Algebra). The first call incurs compilation overhead, but subsequent calls are much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0fa7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a more complex function to see the speedup\n",
    "def dumb_computation(x):\n",
    "    x = jnp.tanh(x @ x.T)\n",
    "    x = jnp.exp(-x**2) @ x\n",
    "    x = jnp.sin(x) + jnp.cos(x.T @ x)\n",
    "    x = jax.nn.softmax(x, axis=-1)\n",
    "    x = jnp.linalg.matrix_power(x, 3)\n",
    "    return jnp.sum(x * jnp.log(jnp.abs(x) + 1e-8))\n",
    "\n",
    "# Create test data\n",
    "test_x = jnp.array(np.random.randn(50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "954b065c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-190.08969, dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = dumb_computation(test_x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2feb0b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674 μs ± 230 μs per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Without jit: interpreted execution\n",
    "%timeit -n 3 -r 3 dumb_computation(test_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83495f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stupid_complex_computation_jit = jax.jit(dumb_computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d7edcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-190.0897, dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_jit = stupid_complex_computation_jit(test_x)\n",
    "out_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4322827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278 μs ± 50.8 μs per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "# With jit: compiled execution\n",
    "%timeit -n 3 -r 3 stupid_complex_computation_jit(test_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e26560",
   "metadata": {},
   "source": [
    "Note the `.block_until_ready()` call—JAX uses asynchronous execution, so we need this to ensure timing is accurate. The compiled version is typically orders of magnitude faster! You can also use `jit` as a decorator: `@jax.jit` above a function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8b5d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Combining transformations: jit a vmapped gradient computation\n",
    "@jax.jit\n",
    "def batched_gradients_optimized(params, x_batch):\n",
    "    return jax.vmap(jax.grad(f, argnums=1), in_axes=(None, 0))(params, x_batch)\n",
    "\n",
    "# This is now fully optimized: compiled + vectorized + differentiated\n",
    "result = batched_gradients_optimized((A, b), x_batch)\n",
    "print(f\"Shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac9d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a58646",
   "metadata": {},
   "source": [
    "# Key Features of JAX for Scientific Computing\n",
    "\n",
    "- **Functional transformations**: `grad()` for gradients, `vmap()` for automatic vectorization, `jit()` for compilation\n",
    "- **Composability**: Transformations can be arbitrarily nested, e.g., `jit(vmap(grad(f)))` to compute batched gradients efficiently\n",
    "- **NumPy compatibility**: Code often requires minimal changes from NumPy, using `jax.numpy` as a drop-in replacement\n",
    "- **Hardware acceleration**: Transparent GPU/TPU execution without code modifications\n",
    "- **Pseudorandom numbers**: Explicit PRNG state management ensures reproducibility in stochastic algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff7f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
